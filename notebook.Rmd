---
title: Literature Review - Climate Smart Ag Links to Ecosystem Services
author: MK Lau
---


Search

https://gofarmhawaii.org/aghelp/

https://attra.ncat.org/topics/

https://www.ams.usda.gov/services/grants




# Context

The City and County of Honolulu's Office of Climate Resilience has
contracted OACA to produce a Climate Smart Agriculture database
intended to inform food system professionals and policy makers on the
potential ecosystem service impacts of CSA activities. The consultant
(MK Lau) has been sub-contracted by OACA to complete the deliverables
listed below in partial fulfillment of the larger contracted
deliverable to HC&C.


# Key Personnel

*Dr. Matthew Kekoa Lau, PhD.* -- will combine his expertise in
ecological systems theory and big data analysis with the practice of
sustainable agriculture to produce the contracted deliverables.

# Deliverables

- CSA-ES literature synthesis
- CSA-ES Resource Database
- CSA Policy 1-pager

# Data Pipeline Design

- Compile search terms 
- Assess search term relevance [litsearchr]
  - Access literature [rcrossref]
- Search literature [EBSCO Search Premier]
- Analyze literature results [bibliometrix]
  - Analyze content for intersection with Hawaiâ€˜i agriculture
  - Key cross-terms: tropic, hawaii, pacific, island, ... 
- Create a dashboard [Shiny]
- Host dashboard [RShiny]

# Search Terms

Terms for practices to focus searches on was generated from the NRCS
CSAF Mitigation Strategies 2024 list:

- [NRCS](https://www.nrcs.usda.gov/sites/default/files/2023-10/NRCS-CSAF-Mitigation-Activities-List.pdf)

Other potential sources of practices could be: 

- [CSA](https://www.nrcs.usda.gov/conservation-basics/natural-resource-concerns/climate/climate-smart-mitigation-activities)
- [CSC](https://www.usda.gov/climate-solutions/climate-smart-commodities "CSC")

Policy information can be obtained from the FAO databases: 

- [FAO](https://fapda.apps.fao.org/fapda/#main.html "FAO Food and Agriculture")
- [FAOForestry](https://www.fao.org/in-action/fapda/forestry-tool/policies/en/ "FAO Forestry Database")

Urban Database for policy information: 

[Urban](https://foodsystemsplanning.ap.buffalo.edu/resources/global-database-for-food-policies/adv-search-gfpd/ "University of Buffalo City and Regional Food Policy Database")

Other government data:

[GovData](https://www.fsa.usda.gov/online-services/fsa-online-data-resources/index "Other Government Data")


```{r setup, echo = FALSE}

library(pacman)
p_load(litsearchr, rcrossref, purrr, wordcloud, 
       tm, RColorBrewer, ggplot2, Rcrawler, rvest, 
       knitr, googlesheets4, dplyr)

```



## rcrossref

https://docs.ropensci.org/rcrossref/articles/rcrossref.html

```{r cr-example, cache = TRUE}

qs <- c("climate smart agriculture")
res <- cr_works(query = qs, limit = 1000) %>% pluck("data")
head(res)

```

```{r csa-wc, echo = FALSE}

rmSW <- function(x, sw, replace = ""){
    if (x %in% sw){
        out <- replace
    }else{
        out <- x
    }
    return(out)
}

## csa.abs <- na.omit(res[["abstract"]]) %>% 
##     paste(collapse = " ")

##     gsub("<[^>]+>", "")

csa.ti <- res[["title"]] %>% 
    tolower %>% 
    gsub(pattern = "[][!#$%()*,.:;<=>@^_|~.{}]", replacement = "")  %>% 
    na.omit %>% 
    strsplit(split = " ")  %>%
    unlist  
csa.ti <- sapply(csa.ti, rmSW, sw = stopwords("english")) 
csa.ti <- csa.ti[csa.ti != ""]
csa.tab <- table(csa.ti)


```{r wc.plot, echo = FALSE}

wordcloud(words = names(csa.tab), freq = csa.tab, min.freq = 10,
          random.order=FALSE, rot.per=0.35, 
          colors=brewer.pal(8, "Dark2"))

```

```{r bar.plot, echo = FALSE}

data <- data.frame(
  category = c("Ecosystem", "Services"),
  value = c(csa.tab["ecosystem"], csa.tab["services"])
)

ggplot(data, aes(x = category, y = value)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(title = "",
       x = "",
       y = "Occurrences")

```


## *litsearchr* workflow

1. Identifies potential keywords through the naive search input
2. Builds a keyword co-occurrence network to increase search precision
3. Uses a cutoff function to identify keyword importance
4. Assists with grouping terms into concepts
5. Writes a Boolean search 

https://luketudge.github.io/litsearchr-tutorial/litsearchr_tutorial.html


```{r litsearchr, cache = TRUE}


```

# analysis

- policy 
- farming practices
- databases
- tropics
- ecosystem services

[Bibliometrix](https://www.bibliometrix.org/home/index.php/layout/bibliometrix  "Bibliometrix")

## R Tools

- https://elizagrames.github.io/litsearchr/#tutorials 
- https://www.bibliometrix.org/home/index.php/layout/bibliometrix 
- https://www.harc-hspa.com/climate-smart-agriculture.html 
- https://github.com/elizagrames/topictagger 
- https://github.com/cran/metagear 
- https://cran.r-project.org/web/packages/scholar/index.html 



# CSA-ES Resource Database

## Webcrawling

https://github.com/salimk/Rcrawler
https://ladal.edu.au/webcrawling.html#Following_links


```{r crawl-nrcs, cahce = TRUE, eval = TRUE}

if ("nrcs-usda-gov-030004.Rdata" %in% dir("./data/")){
    load("data/nrcs-usda-gov-030004.Rdata")
}else{
    Rcrawler(Website = "https://www.nrcs.usda.gov/conservation-basics/natural-resource-concerns/climate/climate-smart-mitigation-activities",
             no_cores = 4, no_conn = 4, 
             NetworkData = TRUE,
             ExtractXpathPat = "//*/a/@href",
             ManyPerPattern = TRUE, MaxDepth = 2, 
             )
}

nrc.url <- INDEX[INDEX[, "Level"] == 2, "Url"]
nrc.url <- nrc.url[grepl("natural-resource-concerns/", nrc.url)]
nrc.cat <- strsplit(nrc.url, "natural-resource-concerns/")
nrc.url <- nrc.url[lapply(nrc.cat, length) == 2]
nrc.cat <- nrc.cat[lapply(nrc.cat, length) == 2]
nrc.cat <- lapply(nrc.cat, function(x) x[2])
nrc.cat <- lapply(nrc.cat, strsplit, split = "\\/")
nrc.cat <- lapply(nrc.cat, unlist)

for (i in seq_len(length(nrc.cat))){
    if (length(nrc.cat[[i]]) < max(unlist(lapply(nrc.cat, length)))){
        nrc.cat[[i]] <- c(nrc.cat[[i]], rep("", times = max(unlist(lapply(nrc.cat, length))) - length(nrc.cat[[i]])))
    }else{}
}

nrc.tab <- cbind(do.call(rbind, nrc.cat), nrc.url)
colnames(nrc.tab) <- c("Category", 
                       paste0("Sub-Category ", seq(1, ncol(nrc.tab)-2)),
                       "Resource")
save(nrc.tab, file = "./data/nrc_tab.Rdata")

```


Data from [NRCS](https://www.nrcs.usda.gov/sites/default/files/2023-10/NRCS-CSAF-Mitigation-Activities-List.pdf)
were extracted manually using [tabula](https://tabula.technology/ "tabula").

```{r nrcs-pdf-tables, }

csa.mit.tab <- read.csv("data/tabula-NRCS-CSAF-Mitigation-Activities.csv", header = FALSE)
csa.mit.head <- csa.mit.tab[grepl("Mitigation Categories", csa.mit.tab[, 1]), ]
csa.mit.head <- gsub("\\[.*?\\]", "", csa.mit.head)
csa.mit.head <- gsub("  ", " ", csa.mit.head)
colnames(csa.mit.tab) <- csa.mit.head
csa.mit.tab <- apply(csa.mit.tab, 2, gsub, pattern = "\\[.*?\\]", replace = "")
csa.mit.tab <- apply(csa.mit.tab, 2, gsub, pattern = "  ", replace = " ")

## Removing narrative crosswalk
csa.cwk <- csa.mit.tab[seq(grep("Waste Storage Structure", csa.mit.tab[, 2]), nrow(csa.mit.tab)), ]
colnames(csa.cwk)[4] <- "Narrative"
csa.mit.tab <-csa.mit.tab[-seq(grep("Waste Storage Structure", csa.mit.tab[, 2]), nrow(csa.mit.tab)), ]

## Generate urls for codes
get.codes <- function(x){
    x <- paste0(x, collapse = " ")
    x <- unlist(strsplit(x, split = " "))
    x <- x[grep("E[0-9][0-9][0-9][a-z,A-Z]", x)]
    return(x)
}

csa.mit.codes <- unlist(lapply(apply(csa.mit.tab, 1, get.codes), function(x) x[1]))
csa.mit.tab[, "Code"] <- csa.mit.codes
csa.mit.tab[!(grepl("E", csa.mit.tab[, "Code"])), "Code"] <- ""
csa.mit.url <- paste0("https://www.nrcs.usda.gov/sites/default/files/2022-11/", 
                      csa.mit.tab[, "Code"],
                      "_July_2022.pdf")
csa.mit.url <- gsub(" ", "-", csa.mit.url)
csa.mit.url[csa.mit.tab[, "Code"] == ""] <- ""
csa.mit.tab <- data.frame(csa.mit.tab, "URL" = csa.mit.url)

```


```{r csa-table, echo=FALSE, message=FALSE, warnings=FALSE, results='asis'}

kable(nrc.tab)

```

```{r mit-table, echo=FALSE, message=FALSE, warnings=FALSE, results='asis'}

kable(csa.mit.tab)

```



## Merge Data Streams

```{r merge-dbs}

db_merge <- function(x, y){
    cn.x <- colnames(x)
    cn.y <- colnames(y)
    cn.xy <- cn.y[!(cn.y %in% cn.x)]
    xy <- data.frame(array(dim = c(nrow(x), length(cn.xy))))
    xy <- data.frame(x, xy)
    colnames(xy) <- c(colnames(x), cn.xy)
    xy <- xy[ ,order(colnames(xy))]
    cn.yx <- cn.x[!(cn.x %in% cn.y)]
    yx <- data.frame(array(dim = c(nrow(y), length(cn.yx))))
    yx <- data.frame(y, yx)
    colnames(yx) <- c(colnames(y), cn.yx)
    yx <- yx[ ,order(colnames(yx))]
    out <- rbind(xy, yx)
    return(out)
}


db.og <- as.data.frame(read_sheet("https://docs.google.com/spreadsheets/d/1AMlsLPDnwt01eEsBLdRe1hvhNSa3ofndcWX0gJ9xbUo/", sheet = "Original"))
db.jh <- as.data.frame(read_sheet("https://docs.google.com/spreadsheets/d/1AMlsLPDnwt01eEsBLdRe1hvhNSa3ofndcWX0gJ9xbUo/", sheet = "Jackson's Version"))

db.mg <- db_merge(db.og, db.jh) %>% 
    distinct

colnames(db.og)[colnames(db.og) == "Resources (Links)"] <- "Resource"
colnames(nrc.tab)[colnames(nrc.tab) == "Sub-Category 1"] <- "Sub-Category"

db.csa <- db_merge(db.og, nrc.tab)


gs4_auth("mklau3@hawaii.edu")
sheet_name <- "https://docs.google.com/spreadsheets/d/1AMlsLPDnwt01eEsBLdRe1hvhNSa3ofndcWX0gJ9xbUo/"
new_sheet <- TRUE  # Set to TRUE if you want to create a new sheet, FALSE otherwise

# Write data frame to Google Sheet
if (new_sheet) {
  # Create a new Google Sheet
    gs4_create(sheet_name, sheets = list(data_frame_name = db.csa))
} else {
  # Specify an existing Google Sheet
    sheet_id <- "your_sheet_id"  # Replace with the ID of your existing Google Sheet
    gs4_get(sheet_id) %>% gs4_write(db.csa, sheet = "Sheet1")
}


```



# Web-hosted Database Interface

## Worflow

- Generate database with known units
- Create webapp using **shiny**
- Deploy to <https://uhwo.shinyapps.io/hicsatool/> using **rsconnect**
  and [shiny.io](shiny.posit.co/r/articles/share/shinyapps)
- Test using [TavisCI](https://www.travis-ci.com/)
- Issues go to <https://github.com/ecoFw/hi-csa-es/issues>

```{r shiny-app, eval = FALSE, echo = TRUE}

runApp()


```


<!-- Rendering rmarkdown Notebook -->
```{r render-notbook, eval = FALSE, echo = FALSE, message=FALSE, warnings=FALSE}

rmarkdown::render("notebook.Rmd", 
                  output_format = c("html_document", "pdf_document", "odt_document"))

```
